{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyODUGWv3GR5GcmlMpj2Trc6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tamara-kostova/MSc_Thesis_Neuroimaging/blob/master/04_models_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "T848RttRIFk_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, confusion_matrix, classification_report\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Upgi0_CYIPIl",
        "outputId": "58ab8e02-1610-4fb0-a185-27f35ac841d8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    \"\"\"Global configuration for all models\"\"\"\n",
        "\n",
        "    # Paths\n",
        "    BASE_DIR = \"/content/drive/MyDrive/MSc_Thesis_Neuroimaging\"\n",
        "    SPLIT_DIR = f\"{BASE_DIR}/data/split\"\n",
        "    RESULTS_DIR = f\"{BASE_DIR}/results/benchmarks\"\n",
        "    CHECKPOINT_DIR = f\"{BASE_DIR}/checkpoints\"\n",
        "\n",
        "    # Training parameters\n",
        "    BATCH_SIZE = 32\n",
        "    NUM_EPOCHS = 5\n",
        "    LEARNING_RATE = 1e-4\n",
        "    WEIGHT_DECAY = 1e-5\n",
        "\n",
        "    # Device\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Random seed\n",
        "    SEED = 42\n",
        "\n",
        "    # Datasets to train on\n",
        "    DATASETS = [\n",
        "        \"MRI_tumor_binary_norm\",\n",
        "        \"MRI_tumor_multiclass_norm\",\n",
        "        \"MRI_ms_norm\",\n",
        "        \"CT_stroke_binary_norm\"\n",
        "    ]\n",
        "\n",
        "    # Models to benchmark\n",
        "    MODELS = [\n",
        "        \"resnet50\",\n",
        "        \"resnet101\",\n",
        "        \"vgg16\",\n",
        "        \"densenet121\",\n",
        "        \"densenet169\",\n",
        "        \"inception_v3\",\n",
        "        \"mobilenet_v2\",\n",
        "        \"efficientnet_b0\",\n",
        "        \"efficientnet_b4\",\n",
        "    ]\n",
        "\n",
        "    # Early stopping\n",
        "    PATIENCE = 10\n",
        "    MIN_DELTA = 1e-3\n",
        "\n",
        "    def __init__(self):\n",
        "        os.makedirs(self.RESULTS_DIR, exist_ok=True)\n",
        "        os.makedirs(self.CHECKPOINT_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "vqpRhUjWIUWm"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MedicalImageDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for medical images with stratified splits\"\"\"\n",
        "\n",
        "    def __init__(self, split_dir, split_type=\"train\", transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            split_dir: path to split directory\n",
        "            split_type: \"train\", \"val\", or \"test\"\n",
        "            transform: image transformations\n",
        "        \"\"\"\n",
        "        self.split_dir = split_dir\n",
        "        self.split_type = split_type\n",
        "        self.transform = transform\n",
        "\n",
        "        self.samples = []\n",
        "        self.class_to_idx = {}\n",
        "        self._build_samples()\n",
        "\n",
        "    def _build_samples(self):\n",
        "        \"\"\"Build list of (path, label) tuples\"\"\"\n",
        "        split_path = os.path.join(self.split_dir, self.split_type)\n",
        "\n",
        "        idx = 0\n",
        "        for class_name in sorted(os.listdir(split_path)):\n",
        "            class_path = os.path.join(split_path, class_name)\n",
        "\n",
        "            if not os.path.isdir(class_path):\n",
        "                continue\n",
        "\n",
        "            if class_name not in self.class_to_idx:\n",
        "                self.class_to_idx[class_name] = idx\n",
        "                idx += 1\n",
        "\n",
        "            label = self.class_to_idx[class_name]\n",
        "\n",
        "            for img_name in os.listdir(class_path):\n",
        "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    img_path = os.path.join(class_path, img_name)\n",
        "                    self.samples.append((img_path, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        from PIL import Image\n",
        "\n",
        "        img_path, label = self.samples[idx]\n",
        "\n",
        "        # Load as grayscale and convert to RGB (3 channels for pretrained models)\n",
        "        image = Image.open(img_path).convert('L')\n",
        "        image_rgb = Image.new('RGB', image.size)\n",
        "        image_rgb.paste(image)\n",
        "\n",
        "        if self.transform:\n",
        "            image_rgb = self.transform(image_rgb)\n",
        "\n",
        "        return image_rgb, label"
      ],
      "metadata": {
        "id": "J1NE1uTFIWlc"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_loaders(split_dir, batch_size=32, num_workers=2):\n",
        "    \"\"\"Create train/val/test DataLoaders\"\"\"\n",
        "\n",
        "    # ImageNet normalization\n",
        "    normalize = transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "\n",
        "    # Training transforms (with augmentation)\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "        transforms.RandomAffine(degrees=5, scale=(0.9, 1.1)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ])\n",
        "\n",
        "    # Val/Test transforms (no augmentation)\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ])\n",
        "\n",
        "    # Create datasets\n",
        "    train_ds = MedicalImageDataset(split_dir, \"train\", train_transform)\n",
        "    val_ds = MedicalImageDataset(split_dir, \"val\", test_transform)\n",
        "    test_ds = MedicalImageDataset(split_dir, \"test\", test_transform)\n",
        "\n",
        "    # Create loaders\n",
        "    loaders = {\n",
        "        'train': DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n",
        "                           num_workers=num_workers, pin_memory=True),\n",
        "        'val': DataLoader(val_ds, batch_size=batch_size, shuffle=False,\n",
        "                         num_workers=num_workers, pin_memory=True),\n",
        "        'test': DataLoader(test_ds, batch_size=batch_size, shuffle=False,\n",
        "                          num_workers=num_workers, pin_memory=True),\n",
        "    }\n",
        "\n",
        "    return loaders, train_ds.class_to_idx"
      ],
      "metadata": {
        "id": "gvFkUlIkIaiE"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(model_name, num_classes, pretrained=True):\n",
        "    \"\"\"Create model with specified architecture\"\"\"\n",
        "\n",
        "    if model_name == \"resnet50\":\n",
        "        model = models.resnet50(pretrained=pretrained)\n",
        "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "    elif model_name == \"resnet101\":\n",
        "        model = models.resnet101(pretrained=pretrained)\n",
        "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "    elif model_name == \"vgg16\":\n",
        "        model = models.vgg16(pretrained=pretrained)\n",
        "        model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
        "\n",
        "    elif model_name == \"vgg19\":\n",
        "        model = models.vgg19(pretrained=pretrained)\n",
        "        model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
        "\n",
        "    elif model_name == \"densenet121\":\n",
        "        model = models.densenet121(pretrained=pretrained)\n",
        "        model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n",
        "\n",
        "    elif model_name == \"densenet169\":\n",
        "        model = models.densenet169(pretrained=pretrained)\n",
        "        model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n",
        "\n",
        "    elif model_name == \"inception_v3\":\n",
        "        model = models.inception_v3(pretrained=pretrained)\n",
        "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "        model.AuxLogits.fc = nn.Linear(model.AuxLogits.fc.in_features, num_classes)\n",
        "\n",
        "    elif model_name == \"mobilenet_v2\":\n",
        "        model = models.mobilenet_v2(pretrained=pretrained)\n",
        "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
        "\n",
        "    elif model_name == \"efficientnet_b0\":\n",
        "        model = models.efficientnet_b0(pretrained=pretrained)\n",
        "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
        "\n",
        "    elif model_name == \"efficientnet_b4\":\n",
        "        model = models.efficientnet_b4(pretrained=pretrained)\n",
        "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model: {model_name}\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "Rf0GY0ixIcSJ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stopping to prevent overfitting\"\"\"\n",
        "\n",
        "    def __init__(self, patience=10, min_delta=0.0, restore_best=True):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.restore_best = restore_best\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.best_epoch = None\n",
        "        self.best_state = None\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "            self.best_state = model.state_dict().copy()\n",
        "            self.best_epoch = 0\n",
        "        elif val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "            self.best_state = model.state_dict().copy()\n",
        "            self.best_epoch = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            self.best_epoch += 1\n",
        "\n",
        "        return self.counter >= self.patience\n",
        "\n",
        "    def restore_best_weights(self, model):\n",
        "        if self.best_state is not None and self.restore_best:\n",
        "            model.load_state_dict(self.best_state)"
      ],
      "metadata": {
        "id": "htHoi5o1IeUP"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    pbar = tqdm(loader, desc=\"Training\", leave=False)\n",
        "    for images, labels in pbar:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            preds = outputs.argmax(dim=1).cpu().numpy()  # [batch_size]\n",
        "            all_preds.append(preds)\n",
        "            all_labels.append(labels.cpu().numpy())  # [batch_size]\n",
        "\n",
        "        pbar.update(1)\n",
        "\n",
        "    avg_loss = total_loss / len(loader.dataset)\n",
        "\n",
        "    all_preds = np.concatenate(all_preds, axis=0)  # [total_samples]\n",
        "    all_labels = np.concatenate(all_labels, axis=0)  # [total_samples]\n",
        "\n",
        "    avg_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    return avg_loss, avg_acc"
      ],
      "metadata": {
        "id": "xp9g5ZZwIgGJ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_epoch(model, loader, criterion, device):\n",
        "    \"\"\"Validate for one epoch\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(loader, desc=\"Validating\", leave=False)\n",
        "        for images, labels in pbar:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "\n",
        "            preds = outputs.argmax(dim=1).cpu().numpy()  # [batch_size]\n",
        "            probs = torch.softmax(outputs, dim=1).cpu().numpy()  # [batch_size, num_classes]\n",
        "\n",
        "            all_preds.append(preds)\n",
        "            all_labels.append(labels.cpu().numpy())  # [batch_size]\n",
        "            all_probs.append(probs)\n",
        "\n",
        "            pbar.update(1)\n",
        "\n",
        "    avg_loss = total_loss / len(loader.dataset)\n",
        "\n",
        "    all_preds = np.concatenate(all_preds, axis=0)  # [total_samples]\n",
        "    all_labels = np.concatenate(all_labels, axis=0)  # [total_samples]\n",
        "    all_probs = np.concatenate(all_probs, axis=0)  # [total_samples, num_classes]\n",
        "\n",
        "    avg_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    # Compute AUC\n",
        "    try:\n",
        "        if len(np.unique(all_labels)) == 2:\n",
        "            # Binary: use positive class probabilities (class 1)\n",
        "            avg_auc = roc_auc_score(all_labels, all_probs[:, 1])\n",
        "        else:\n",
        "            # Multiclass: one-vs-rest\n",
        "            avg_auc = roc_auc_score(all_labels, all_probs, multi_class='ovr')\n",
        "    except Exception as e:\n",
        "        print(f\"  Warning: Could not compute AUC: {e}\")\n",
        "        avg_auc = 0.0\n",
        "\n",
        "    return avg_loss, avg_acc, avg_auc"
      ],
      "metadata": {
        "id": "zMke-MlcIh0W"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, loaders, criterion, optimizer, scheduler, device,\n",
        "                num_epochs, model_name, dataset_name, checkpoint_dir):\n",
        "    \"\"\"Train model with early stopping\"\"\"\n",
        "\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, f\"{model_name}_{dataset_name}.pt\")\n",
        "\n",
        "    early_stop = EarlyStopping(patience=Config.PATIENCE, min_delta=Config.MIN_DELTA)\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [], 'train_acc': [],\n",
        "        'val_loss': [], 'val_acc': [], 'val_auc': []\n",
        "    }\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Training {model_name} on {dataset_name}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_acc = train_epoch(\n",
        "            model, loaders['train'], criterion, optimizer, device\n",
        "        )\n",
        "\n",
        "        val_loss, val_acc, val_auc = validate_epoch(\n",
        "            model, loaders['val'], criterion, device\n",
        "        )\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        history['val_auc'].append(val_auc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1:3d}/{num_epochs} | \"\n",
        "              f\"TrLoss: {train_loss:.4f} | TrAcc: {train_acc:.4f} | \"\n",
        "              f\"VaLoss: {val_loss:.4f} | VaAcc: {val_acc:.4f} | VaAUC: {val_auc:.4f}\")\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        if scheduler is not None:\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "        # Early stopping\n",
        "        if early_stop(val_loss, model):\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            early_stop.restore_best_weights(model)\n",
        "            break\n",
        "\n",
        "        # Save checkpoint\n",
        "        if epoch % 5 == 0:\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "\n",
        "    return history, checkpoint_path"
      ],
      "metadata": {
        "id": "HtsDtASwIjr4"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, loader, device):\n",
        "    \"\"\"Full evaluation metrics\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "\n",
        "            preds = outputs.argmax(dim=1).cpu().numpy()\n",
        "            probs = torch.softmax(outputs, dim=1).cpu().numpy()\n",
        "\n",
        "            all_preds.append(preds)\n",
        "            all_labels.append(labels.cpu().numpy())\n",
        "            all_probs.append(probs)\n",
        "\n",
        "    all_preds = np.concatenate(all_preds, axis=0)\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "    all_probs = np.concatenate(all_probs, axis=0)\n",
        "\n",
        "    # Compute metrics\n",
        "    metrics = {\n",
        "        'accuracy': accuracy_score(all_labels, all_preds),\n",
        "        'precision': precision_score(all_labels, all_preds, average='weighted', zero_division=0),\n",
        "        'recall': recall_score(all_labels, all_preds, average='weighted', zero_division=0),\n",
        "        'f1': f1_score(all_labels, all_preds, average='weighted', zero_division=0),\n",
        "    }\n",
        "\n",
        "    # AUC\n",
        "    try:\n",
        "        if len(np.unique(all_labels)) == 2:\n",
        "            # Binary: use positive class probabilities (class 1)\n",
        "            metrics['auc'] = roc_auc_score(all_labels, all_probs[:, 1])\n",
        "        else:\n",
        "            # Multiclass: one-vs-rest\n",
        "            metrics['auc'] = roc_auc_score(all_labels, all_probs, multi_class='ovr')\n",
        "    except Exception as e:\n",
        "        print(f\"  Warning: Could not compute AUC: {e}\")\n",
        "        metrics['auc'] = 0.0\n",
        "\n",
        "    return metrics, all_preds, all_labels"
      ],
      "metadata": {
        "id": "Hjgfsy6pIlR1"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_results(results, output_path):\n",
        "    \"\"\"Save results to JSON\"\"\"\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "    with open(output_path, 'w') as f:\n",
        "        json.dump(results, f, indent=2, default=str)"
      ],
      "metadata": {
        "id": "c59mi0s-InOF"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = Config()\n",
        "torch.manual_seed(config.SEED)\n",
        "np.random.seed(config.SEED)\n",
        "all_results = {}"
      ],
      "metadata": {
        "id": "Rf4CN6fRIoiG"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for dataset_name in config.DATASETS:\n",
        "        dataset_path = os.path.join(config.SPLIT_DIR, dataset_name)\n",
        "\n",
        "        if not os.path.exists(dataset_path):\n",
        "            print(f\"Dataset not found: {dataset_path}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n\\n{'#'*70}\")\n",
        "        print(f\"# DATASET: {dataset_name}\")\n",
        "        print(f\"{'#'*70}\")\n",
        "\n",
        "        # Load data\n",
        "        loaders, class_to_idx = get_data_loaders(\n",
        "            dataset_path,\n",
        "            batch_size=config.BATCH_SIZE,\n",
        "            num_workers=2\n",
        "        )\n",
        "\n",
        "        num_classes = len(class_to_idx)\n",
        "        print(f\"Number of classes: {num_classes}\")\n",
        "        print(f\"Classes: {list(class_to_idx.keys())}\")\n",
        "\n",
        "        dataset_results = {}\n",
        "\n",
        "        # Iterate over models\n",
        "        for model_name in config.MODELS:\n",
        "            try:\n",
        "                print(f\"\\n--- Training {model_name} ---\")\n",
        "\n",
        "                # Create model\n",
        "                model = create_model(model_name, num_classes, pretrained=True)\n",
        "                model = model.to(config.DEVICE)\n",
        "\n",
        "                # Count parameters\n",
        "                total_params = sum(p.numel() for p in model.parameters())\n",
        "                trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "                print(f\"Parameters: {total_params/1e6:.2f}M (trainable: {trainable_params/1e6:.2f}M)\")\n",
        "\n",
        "                # Loss function\n",
        "                criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "                # Optimizer\n",
        "                optimizer = optim.AdamW(\n",
        "                    model.parameters(),\n",
        "                    lr=config.LEARNING_RATE,\n",
        "                    weight_decay=config.WEIGHT_DECAY\n",
        "                )\n",
        "\n",
        "                # Scheduler\n",
        "                scheduler = ReduceLROnPlateau(\n",
        "                    optimizer, mode='min', factor=0.5, patience=5\n",
        "                )\n",
        "\n",
        "                # Train\n",
        "                history, checkpoint_path = train_model(\n",
        "                    model, loaders, criterion, optimizer, scheduler, config.DEVICE,\n",
        "                    config.NUM_EPOCHS, model_name, dataset_name, config.CHECKPOINT_DIR\n",
        "                )\n",
        "\n",
        "                # Load best model\n",
        "                model.load_state_dict(torch.load(checkpoint_path, map_location=config.DEVICE))\n",
        "\n",
        "                # Evaluate\n",
        "                test_metrics, _, _ = evaluate_model(model, loaders['test'], config.DEVICE)\n",
        "\n",
        "                print(f\"\\nTest Results:\")\n",
        "                for metric, value in test_metrics.items():\n",
        "                    print(f\"  {metric}: {value:.4f}\")\n",
        "\n",
        "                dataset_results[model_name] = {\n",
        "                    'test_metrics': test_metrics,\n",
        "                    'history': history,\n",
        "                    'params': trainable_params,\n",
        "                }\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error training {model_name}: {str(e)}\")\n",
        "                dataset_results[model_name] = {'error': str(e)}\n",
        "\n",
        "        all_results[dataset_name] = dataset_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oylPQ_ZqIqg1",
        "outputId": "61fca067-64d5-4219-a6f5-0b00c7dc42bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "######################################################################\n",
            "# DATASET: MRI_tumor_binary_norm\n",
            "######################################################################\n",
            "Number of classes: 2\n",
            "Classes: ['normal', 'tumor']\n",
            "\n",
            "--- Training resnet50 ---\n",
            "Parameters: 23.51M (trainable: 23.51M)\n",
            "\n",
            "======================================================================\n",
            "Training resnet50 on MRI_tumor_binary_norm\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   1/5 | TrLoss: 0.1662 | TrAcc: 0.9343 | VaLoss: 0.0391 | VaAcc: 0.9867 | VaAUC: 0.9988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   2/5 | TrLoss: 0.0568 | TrAcc: 0.9829 | VaLoss: 0.0074 | VaAcc: 0.9978 | VaAUC: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   3/5 | TrLoss: 0.0324 | TrAcc: 0.9886 | VaLoss: 0.0083 | VaAcc: 0.9978 | VaAUC: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   4/5 | TrLoss: 0.0073 | TrAcc: 0.9971 | VaLoss: 0.0112 | VaAcc: 0.9978 | VaAUC: 0.9999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   5/5 | TrLoss: 0.0176 | TrAcc: 0.9952 | VaLoss: 0.0097 | VaAcc: 0.9978 | VaAUC: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Results:\n",
            "  accuracy: 0.9822\n",
            "  precision: 0.9824\n",
            "  recall: 0.9822\n",
            "  f1: 0.9822\n",
            "  auc: 0.9991\n",
            "\n",
            "--- Training resnet101 ---\n",
            "Parameters: 42.50M (trainable: 42.50M)\n",
            "\n",
            "======================================================================\n",
            "Training resnet101 on MRI_tumor_binary_norm\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   1/5 | TrLoss: 0.1481 | TrAcc: 0.9390 | VaLoss: 0.0574 | VaAcc: 0.9867 | VaAUC: 0.9963\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   2/5 | TrLoss: 0.0555 | TrAcc: 0.9857 | VaLoss: 0.0809 | VaAcc: 0.9822 | VaAUC: 0.9980\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   3/5 | TrLoss: 0.0465 | TrAcc: 0.9852 | VaLoss: 0.1446 | VaAcc: 0.9844 | VaAUC: 0.9929\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   4/5 | TrLoss: 0.0312 | TrAcc: 0.9919 | VaLoss: 0.0067 | VaAcc: 0.9978 | VaAUC: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   5/5 | TrLoss: 0.0256 | TrAcc: 0.9924 | VaLoss: 0.0136 | VaAcc: 0.9933 | VaAUC: 0.9999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Results:\n",
            "  accuracy: 0.9956\n",
            "  precision: 0.9956\n",
            "  recall: 0.9956\n",
            "  f1: 0.9956\n",
            "  auc: 0.9993\n",
            "\n",
            "--- Training vgg16 ---\n",
            "Parameters: 134.27M (trainable: 134.27M)\n",
            "\n",
            "======================================================================\n",
            "Training vgg16 on MRI_tumor_binary_norm\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   1/5 | TrLoss: 0.3378 | TrAcc: 0.8638 | VaLoss: 0.0917 | VaAcc: 0.9689 | VaAUC: 0.9964\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   2/5 | TrLoss: 0.1178 | TrAcc: 0.9719 | VaLoss: 0.0800 | VaAcc: 0.9822 | VaAUC: 0.9986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   3/5 | TrLoss: 0.0662 | TrAcc: 0.9838 | VaLoss: 0.1331 | VaAcc: 0.9844 | VaAUC: 0.9972\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   4/5 | TrLoss: 0.0621 | TrAcc: 0.9829 | VaLoss: 0.0318 | VaAcc: 0.9867 | VaAUC: 0.9999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   5/5 | TrLoss: 0.0440 | TrAcc: 0.9895 | VaLoss: 0.1014 | VaAcc: 0.9844 | VaAUC: 0.9995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Results:\n",
            "  accuracy: 0.9800\n",
            "  precision: 0.9801\n",
            "  recall: 0.9800\n",
            "  f1: 0.9800\n",
            "  auc: 0.9991\n",
            "\n",
            "--- Training densenet121 ---\n",
            "Parameters: 6.96M (trainable: 6.96M)\n",
            "\n",
            "======================================================================\n",
            "Training densenet121 on MRI_tumor_binary_norm\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   1/5 | TrLoss: 0.2146 | TrAcc: 0.9143 | VaLoss: 0.0477 | VaAcc: 0.9867 | VaAUC: 0.9992\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   2/5 | TrLoss: 0.0423 | TrAcc: 0.9876 | VaLoss: 0.0203 | VaAcc: 0.9978 | VaAUC: 0.9996\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   3/5 | TrLoss: 0.0245 | TrAcc: 0.9914 | VaLoss: 0.0143 | VaAcc: 0.9956 | VaAUC: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   4/5 | TrLoss: 0.0173 | TrAcc: 0.9943 | VaLoss: 0.0120 | VaAcc: 0.9978 | VaAUC: 0.9998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   5/5 | TrLoss: 0.0117 | TrAcc: 0.9971 | VaLoss: 0.0045 | VaAcc: 0.9978 | VaAUC: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Results:\n",
            "  accuracy: 0.9778\n",
            "  precision: 0.9779\n",
            "  recall: 0.9778\n",
            "  f1: 0.9778\n",
            "  auc: 0.9992\n",
            "\n",
            "--- Training densenet169 ---\n",
            "Parameters: 12.49M (trainable: 12.49M)\n",
            "\n",
            "======================================================================\n",
            "Training densenet169 on MRI_tumor_binary_norm\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   1/5 | TrLoss: 0.1632 | TrAcc: 0.9414 | VaLoss: 0.0247 | VaAcc: 0.9911 | VaAUC: 0.9998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   2/5 | TrLoss: 0.0447 | TrAcc: 0.9871 | VaLoss: 0.0191 | VaAcc: 0.9933 | VaAUC: 0.9998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   3/5 | TrLoss: 0.0118 | TrAcc: 0.9967 | VaLoss: 0.0222 | VaAcc: 0.9911 | VaAUC: 0.9998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   4/5 | TrLoss: 0.0302 | TrAcc: 0.9910 | VaLoss: 0.0095 | VaAcc: 0.9956 | VaAUC: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   5/5 | TrLoss: 0.0116 | TrAcc: 0.9957 | VaLoss: 0.0232 | VaAcc: 0.9889 | VaAUC: 0.9997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Results:\n",
            "  accuracy: 0.9822\n",
            "  precision: 0.9823\n",
            "  recall: 0.9822\n",
            "  f1: 0.9822\n",
            "  auc: 0.9972\n",
            "\n",
            "--- Training inception_v3 ---\n",
            "Parameters: 24.35M (trainable: 24.35M)\n",
            "\n",
            "======================================================================\n",
            "Training inception_v3 on MRI_tumor_binary_norm\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error training inception_v3: Calculated padded input size per channel: (3 x 3). Kernel size: (5 x 5). Kernel size can't be greater than actual input size\n",
            "\n",
            "--- Training mobilenet_v2 ---\n",
            "Parameters: 2.23M (trainable: 2.23M)\n",
            "\n",
            "======================================================================\n",
            "Training mobilenet_v2 on MRI_tumor_binary_norm\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   1/5 | TrLoss: 0.2277 | TrAcc: 0.9057 | VaLoss: 0.0692 | VaAcc: 0.9800 | VaAUC: 0.9975\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   2/5 | TrLoss: 0.0650 | TrAcc: 0.9800 | VaLoss: 0.0450 | VaAcc: 0.9844 | VaAUC: 0.9990\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   3/5 | TrLoss: 0.0316 | TrAcc: 0.9890 | VaLoss: 0.0513 | VaAcc: 0.9844 | VaAUC: 0.9981\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   4/5 | TrLoss: 0.0285 | TrAcc: 0.9929 | VaLoss: 0.0353 | VaAcc: 0.9889 | VaAUC: 0.9997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   5/5 | TrLoss: 0.0269 | TrAcc: 0.9933 | VaLoss: 0.0373 | VaAcc: 0.9889 | VaAUC: 0.9995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Results:\n",
            "  accuracy: 0.9867\n",
            "  precision: 0.9867\n",
            "  recall: 0.9867\n",
            "  f1: 0.9867\n",
            "  auc: 0.9986\n",
            "\n",
            "--- Training efficientnet_b0 ---\n",
            "Parameters: 4.01M (trainable: 4.01M)\n",
            "\n",
            "======================================================================\n",
            "Training efficientnet_b0 on MRI_tumor_binary_norm\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  42%|████▏     | 28/66 [00:04<00:07,  5.37it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_path = os.path.join(config.RESULTS_DIR, f\"benchmark_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\")\n",
        "save_results(all_results, results_path)\n",
        "\n",
        "    # Print summary\n",
        "print(f\"\\n\\n{'#'*70}\")\n",
        "print(f\"# SUMMARY\")\n",
        "print(f\"{'#'*70}\")\n",
        "\n",
        "summary_df = []\n",
        "for dataset_name, models in all_results.items():\n",
        "    for model_name, results in models.items():\n",
        "        if 'test_metrics' in results:\n",
        "            row = {\n",
        "                'dataset': dataset_name,\n",
        "                'model': model_name,\n",
        "                'accuracy': results['test_metrics']['accuracy'],\n",
        "                'f1': results['test_metrics']['f1'],\n",
        "                'auc': results['test_metrics']['auc'],\n",
        "            }\n",
        "            summary_df.append(row)\n",
        "\n",
        "summary_df = pd.DataFrame(summary_df)\n",
        "summary_df = summary_df.sort_values('accuracy', ascending=False)\n",
        "\n",
        "print(\"\\nTop Results (by Accuracy):\")\n",
        "print(summary_df.head(10).to_string(index=False))\n",
        "\n",
        "# Save summary\n",
        "summary_path = os.path.join(config.RESULTS_DIR, f\"summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\")\n",
        "summary_df.to_csv(summary_path, index=False)\n",
        "\n",
        "print(f\"\\nResults saved to: {results_path}\")\n",
        "print(f\"Summary saved to: {summary_path}\")"
      ],
      "metadata": {
        "id": "OX96JZOAJBXP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}