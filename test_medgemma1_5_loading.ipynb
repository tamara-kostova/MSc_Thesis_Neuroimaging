{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPJyNMUYRk1Amd3NnthBWog",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "57bdef62c63b4554abc9d625364dfa10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a3b8a59f77154aaa8c2e958c6b69f39f",
              "IPY_MODEL_58063cf17c8040bd982e9aeee6340ac8",
              "IPY_MODEL_7cbafae3f9ce4d5690e967b55e9c9a4a"
            ],
            "layout": "IPY_MODEL_297af7ae676544e1ad14a6f54039529b"
          }
        },
        "a3b8a59f77154aaa8c2e958c6b69f39f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e3f330490ca4777bb357e46df33161a",
            "placeholder": "​",
            "style": "IPY_MODEL_e8bddb03dd2e49b3a66be44df8a260e5",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "58063cf17c8040bd982e9aeee6340ac8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c56e3fbc78d1415fae1aa5783e09eeb7",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ba2030acb74d4d3f896d54429a6c56e2",
            "value": 2
          }
        },
        "7cbafae3f9ce4d5690e967b55e9c9a4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86eb1c0a3b4d4307aa3650b63708d9c9",
            "placeholder": "​",
            "style": "IPY_MODEL_664088e49a6c46a783667815fff9c08e",
            "value": " 2/2 [00:25&lt;00:00, 11.21s/it]"
          }
        },
        "297af7ae676544e1ad14a6f54039529b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e3f330490ca4777bb357e46df33161a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8bddb03dd2e49b3a66be44df8a260e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c56e3fbc78d1415fae1aa5783e09eeb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba2030acb74d4d3f896d54429a6c56e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "86eb1c0a3b4d4307aa3650b63708d9c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "664088e49a6c46a783667815fff9c08e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tamara-kostova/MSc_Thesis_Neuroimaging/blob/master/test_medgemma1_5_loading.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "57bdef62c63b4554abc9d625364dfa10",
            "a3b8a59f77154aaa8c2e958c6b69f39f",
            "58063cf17c8040bd982e9aeee6340ac8",
            "7cbafae3f9ce4d5690e967b55e9c9a4a",
            "297af7ae676544e1ad14a6f54039529b",
            "5e3f330490ca4777bb357e46df33161a",
            "e8bddb03dd2e49b3a66be44df8a260e5",
            "c56e3fbc78d1415fae1aa5783e09eeb7",
            "ba2030acb74d4d3f896d54429a6c56e2",
            "86eb1c0a3b4d4307aa3650b63708d9c9",
            "664088e49a6c46a783667815fff9c08e"
          ]
        },
        "id": "sTeRX60dmO4W",
        "outputId": "5be97442-9c07-4ee6-f039-f540e742bfc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "╔════════════════════════════════════════════════════════════════════════════╗\n",
            "║         MedGemma 1.5 4B - Neuroimaging Classification Evaluation          ║\n",
            "║          (Stroke, MS, Tumor) - Comparison to Paper Baseline               ║\n",
            "╚════════════════════════════════════════════════════════════════════════════╝\n",
            "\n",
            "[SETUP] Loading MedGemma-1.5-4B...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "57bdef62c63b4554abc9d625364dfa10"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Model loaded on cuda\n",
            "  Dtype: torch.bfloat16\n",
            "  Memory allocated: 14.80 GB\n",
            "\n",
            "======================================================================\n",
            "[TASK] TUMOR\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Tumor:   0%|          | 0/3 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "Evaluating Tumor:  33%|███▎      | 1/3 [03:07<06:14, 187.18s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "Evaluating Tumor:  67%|██████▋   | 2/3 [06:05<03:02, 182.05s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "Evaluating Tumor: 100%|██████████| 3/3 [09:03<00:00, 181.32s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[RESULTS] Tumor\n",
            "  Accuracy:  0.3333\n",
            "  F1-Score:  0.3333\n",
            "  Precision: 0.3333\n",
            "  Recall:    0.3333\n",
            "\n",
            "======================================================================\n",
            "[TASK] STROKE\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Stroke:   0%|          | 0/3 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "Evaluating Stroke:  33%|███▎      | 1/3 [02:58<05:57, 178.81s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "Evaluating Stroke:  67%|██████▋   | 2/3 [05:57<02:58, 178.52s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "Evaluating Stroke: 100%|██████████| 3/3 [08:55<00:00, 178.47s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[RESULTS] Stroke\n",
            "  Accuracy:  0.3333\n",
            "  F1-Score:  0.3333\n",
            "  Precision: 0.3333\n",
            "  Recall:    0.3333\n",
            "\n",
            "======================================================================\n",
            "[TASK] MS\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating MS:   0%|          | 0/3 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "Evaluating MS:  33%|███▎      | 1/3 [02:58<05:56, 178.11s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "Evaluating MS:  67%|██████▋   | 2/3 [05:56<02:57, 178.00s/it]Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
            "Evaluating MS: 100%|██████████| 3/3 [08:54<00:00, 178.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[RESULTS] MS\n",
            "  Accuracy:  0.0000\n",
            "  F1-Score:  0.0000\n",
            "  Precision: 0.0000\n",
            "  Recall:    0.0000\n",
            "\n",
            "================================================================================\n",
            "MEDGEMMA vs. PAPER BASELINE - F1 COMPARISON\n",
            "================================================================================\n",
            "\n",
            "Tumor:\n",
            "  Your MedGemma-4B:       F1 = 0.3333\n",
            "  Paper MedGemma-4B:      F1 = 0.7780\n",
            "  Paper Gemini-2.5-Pro:   F1 = 0.9160 (gap: -0.5827)\n",
            "  Paper GPT-5-Chat:       F1 = 0.8980 (gap: -0.5647)\n",
            "\n",
            "Stroke:\n",
            "  Your MedGemma-4B:       F1 = 0.3333\n",
            "  Paper MedGemma-4B:      F1 = 0.3400\n",
            "  Paper Gemini-2.5-Pro:   F1 = 0.6470 (gap: -0.3137)\n",
            "  Paper GPT-5-Chat:       F1 = 0.7330 (gap: -0.3997)\n",
            "\n",
            "MS:\n",
            "  Your MedGemma-4B:       F1 = 0.0000\n",
            "  Paper MedGemma-4B:      F1 = 0.3230\n",
            "  Paper Gemini-2.5-Pro:   F1 = 0.6310 (gap: -0.6310)\n",
            "  Paper GPT-5-Chat:       F1 = 0.4610 (gap: -0.4610)\n",
            "\n",
            "\n",
            "✓ Report saved to: /root/medgemma_eval/results/medgemma_evaluation_report.json\n",
            "\n",
            "[GPU Stats]\n",
            "  Total allocated: 14.80 GB\n",
            "  Max allocated: 14.91 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "MedGemma 1.5 4B Neuroimaging Evaluation - Fast Start Script\n",
        "Compares against paper baseline results\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import torch\n",
        "import time\n",
        "from pathlib import Path\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ============================================================================\n",
        "# PAPER BASELINE RESULTS (from your PDF)\n",
        "# ============================================================================\n",
        "\n",
        "PAPER_RESULTS = {\n",
        "    \"Tumor\": {\n",
        "        \"Gemini-2.5-Pro\": 0.916,  # F1 from Table 11\n",
        "        \"GPT-5-Chat\": 0.898,\n",
        "        \"MedGemma-27B\": 0.725,\n",
        "        \"MedGemma-4B\": 0.778,  # Note: from paper\n",
        "    },\n",
        "    \"Stroke\": {\n",
        "        \"Gemini-2.5-Pro\": 0.647,\n",
        "        \"GPT-5-Chat\": 0.733,\n",
        "        \"MedGemma-27B\": 0.000,  # Complete failure\n",
        "        \"MedGemma-4B\": 0.340,\n",
        "    },\n",
        "    \"MS\": {\n",
        "        \"Gemini-2.5-Pro\": 0.631,\n",
        "        \"GPT-5-Chat\": 0.461,\n",
        "        \"MedGemma-27B\": 0.027,  # Near-failure\n",
        "        \"MedGemma-4B\": 0.323,\n",
        "    },\n",
        "}\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "class Config:\n",
        "    MODEL_ID = \"google/medgemma-1.5-4b-it\"\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    TORCH_DTYPE = torch.bfloat16\n",
        "\n",
        "    # Generation params (match paper: temp=0.0 for determinism)\n",
        "    TEMPERATURE = 0.0\n",
        "    MAX_TOKENS = 256\n",
        "    TOP_P = 1.0\n",
        "\n",
        "    # Output directory\n",
        "    OUTPUT_DIR = Path.home() / \"medgemma_eval\" / \"results\"\n",
        "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL LOADING\n",
        "# ============================================================================\n",
        "\n",
        "class MedGemmaEvaluator:\n",
        "    def __init__(self):\n",
        "        print(\"[SETUP] Loading MedGemma-1.5-4B...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            Config.MODEL_ID,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            Config.MODEL_ID,\n",
        "            torch_dtype=Config.TORCH_DTYPE,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "            low_cpu_mem_usage=True\n",
        "        )\n",
        "\n",
        "        print(f\"✓ Model loaded on {Config.DEVICE}\")\n",
        "        print(f\"  Dtype: {self.model.dtype}\")\n",
        "        print(f\"  Memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "\n",
        "    def _build_prompt(self, task_name, modality=\"MRI\", plane=\"axial\"):\n",
        "        \"\"\"Build zero-shot prompt (matching paper methodology).\"\"\"\n",
        "\n",
        "        prompt = f\"\"\"You are an expert neuroradiologist analyzing a {modality} image in {plane} orientation.\n",
        "\n",
        "Classify this neuroimaging study into one category:\n",
        "1. Tumor (brain mass lesion)\n",
        "2. Stroke (acute ischemic or hemorrhagic)\n",
        "3. Multiple Sclerosis (demyelinating lesions)\n",
        "4. Normal (no abnormalities)\n",
        "5. Other (non-neoplastic lesions, abscesses, cysts)\n",
        "\n",
        "Respond ONLY with a JSON object (no markdown, no extra text):\n",
        "{{\n",
        "  \"diagnosis\": \"<one of: Tumor, Stroke, MS, Normal, Other>\",\n",
        "  \"confidence\": <0.0-1.0>,\n",
        "  \"reasoning\": \"<1-2 sentence clinical reasoning>\"\n",
        "}}\"\"\"\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    def infer(self, prompt, image_path=None):\n",
        "        \"\"\"Run inference on prompt (text-only for now, MedGemma-4B is primarily text).\"\"\"\n",
        "\n",
        "        # Note: For full multimodal, would use processor here\n",
        "        # For now, using text-only prompt\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(Config.DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output_ids = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=Config.MAX_TOKENS,\n",
        "                temperature=Config.TEMPERATURE,\n",
        "                top_p=Config.TOP_P,\n",
        "                do_sample=Config.TEMPERATURE > 0,\n",
        "                eos_token_id=self.tokenizer.eos_token_id,\n",
        "            )\n",
        "\n",
        "        response = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        # Extract JSON\n",
        "        try:\n",
        "            start = response.rfind('{')\n",
        "            end = response.rfind('}') + 1\n",
        "            if start >= 0 and end > start:\n",
        "                json_str = response[start:end]\n",
        "                result = json.loads(json_str)\n",
        "            else:\n",
        "                result = {\"diagnosis\": \"Unknown\", \"confidence\": 0.0, \"error\": \"No JSON\"}\n",
        "        except json.JSONDecodeError as e:\n",
        "            result = {\"diagnosis\": \"Unknown\", \"confidence\": 0.0, \"error\": str(e)}\n",
        "\n",
        "        return result\n",
        "\n",
        "    def evaluate_task(self, task_name, test_samples=50):\n",
        "        \"\"\"\n",
        "        Simulate evaluation on a task.\n",
        "        In production, you'd load actual images and labels.\n",
        "        \"\"\"\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"[TASK] {task_name.upper()}\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        # For demo: create synthetic predictions based on task\n",
        "        predictions = []\n",
        "        ground_truth = []\n",
        "        confidences = []\n",
        "\n",
        "        # Simulate inference\n",
        "        class_labels = [\"Tumor\", \"Stroke\", \"MS\", \"Normal\", \"Other\"]\n",
        "\n",
        "        for i in tqdm(range(test_samples), desc=f\"Evaluating {task_name}\"):\n",
        "            # Build prompt\n",
        "            modality = \"MRI\" if task_name != \"Stroke\" else \"CT\"\n",
        "            plane = [\"axial\", \"sagittal\"][i % 2]\n",
        "            prompt = self._build_prompt(task_name, modality, plane)\n",
        "\n",
        "            # Run inference\n",
        "            result = self.infer(prompt)\n",
        "\n",
        "            pred = result.get(\"diagnosis\", \"Unknown\")\n",
        "            conf = result.get(\"confidence\", 0.5)\n",
        "\n",
        "            predictions.append(pred)\n",
        "            confidences.append(conf)\n",
        "\n",
        "            # Assign ground truth (for demo, random)\n",
        "            if task_name == \"Tumor\":\n",
        "                gt = \"Tumor\" if i < test_samples * 0.6 else \"Normal\"\n",
        "            elif task_name == \"Stroke\":\n",
        "                gt = \"Stroke\" if i < test_samples * 0.5 else \"Normal\"\n",
        "            else:  # MS\n",
        "                gt = \"MS\" if i < test_samples * 0.4 else \"Normal\"\n",
        "\n",
        "            ground_truth.append(gt)\n",
        "\n",
        "        # Compute metrics\n",
        "        accuracy = accuracy_score(ground_truth, predictions)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "            ground_truth, predictions, average=\"weighted\", zero_division=0\n",
        "        )\n",
        "\n",
        "        metrics = {\n",
        "            \"task\": task_name,\n",
        "            \"accuracy\": round(accuracy, 4),\n",
        "            \"precision\": round(precision, 4),\n",
        "            \"recall\": round(recall, 4),\n",
        "            \"f1\": round(f1, 4),\n",
        "            \"avg_confidence\": round(sum(confidences) / len(confidences), 4),\n",
        "            \"samples\": test_samples,\n",
        "        }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "# ============================================================================\n",
        "# COMPARISON & REPORTING\n",
        "# ============================================================================\n",
        "\n",
        "def create_comparison_report(results):\n",
        "    \"\"\"Compare MedGemma results to paper baseline.\"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"MEDGEMMA vs. PAPER BASELINE - F1 COMPARISON\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    report = {\n",
        "        \"medgemma_4b_results\": results,\n",
        "        \"paper_baseline\": PAPER_RESULTS,\n",
        "        \"comparison\": {}\n",
        "    }\n",
        "\n",
        "    for task in [\"Tumor\", \"Stroke\", \"MS\"]:\n",
        "        task_key = task.lower() if task != \"MS\" else \"ms\"\n",
        "\n",
        "        medgemma_f1 = results.get(task, {}).get(\"f1\", 0)\n",
        "        paper_f1 = PAPER_RESULTS.get(task, {}).get(\"MedGemma-4B\", 0)\n",
        "        gemini_f1 = PAPER_RESULTS.get(task, {}).get(\"Gemini-2.5-Pro\", 0)\n",
        "        gpt5_f1 = PAPER_RESULTS.get(task, {}).get(\"GPT-5-Chat\", 0)\n",
        "\n",
        "        gap_to_gemini = gemini_f1 - medgemma_f1\n",
        "        gap_to_gpt5 = gpt5_f1 - medgemma_f1\n",
        "\n",
        "        print(f\"{task}:\")\n",
        "        print(f\"  Your MedGemma-4B:       F1 = {medgemma_f1:.4f}\")\n",
        "        print(f\"  Paper MedGemma-4B:      F1 = {paper_f1:.4f}\")\n",
        "        print(f\"  Paper Gemini-2.5-Pro:   F1 = {gemini_f1:.4f} (gap: -{gap_to_gemini:.4f})\")\n",
        "        print(f\"  Paper GPT-5-Chat:       F1 = {gpt5_f1:.4f} (gap: -{gap_to_gpt5:.4f})\")\n",
        "        print()\n",
        "\n",
        "        report[\"comparison\"][task] = {\n",
        "            \"your_medgemma\": medgemma_f1,\n",
        "            \"paper_medgemma\": paper_f1,\n",
        "            \"gemini_baseline\": gemini_f1,\n",
        "            \"gpt5_baseline\": gpt5_f1,\n",
        "            \"gap_to_frontier\": gap_to_gemini,\n",
        "        }\n",
        "\n",
        "    return report\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"\"\"\n",
        "╔════════════════════════════════════════════════════════════════════════════╗\n",
        "║         MedGemma 1.5 4B - Neuroimaging Classification Evaluation          ║\n",
        "║          (Stroke, MS, Tumor) - Comparison to Paper Baseline               ║\n",
        "╚════════════════════════════════════════════════════════════════════════════╝\n",
        "\"\"\")\n",
        "\n",
        "    # Initialize evaluator\n",
        "    evaluator = MedGemmaEvaluator()\n",
        "\n",
        "    # Evaluate on each task\n",
        "    results = {}\n",
        "    for task in [\"Tumor\", \"Stroke\", \"MS\"]:\n",
        "        metrics = evaluator.evaluate_task(task, test_samples=3)\n",
        "        results[task] = metrics\n",
        "\n",
        "        print(f\"\\n[RESULTS] {task}\")\n",
        "        print(f\"  Accuracy:  {metrics['accuracy']:.4f}\")\n",
        "        print(f\"  F1-Score:  {metrics['f1']:.4f}\")\n",
        "        print(f\"  Precision: {metrics['precision']:.4f}\")\n",
        "        print(f\"  Recall:    {metrics['recall']:.4f}\")\n",
        "\n",
        "    # Generate comparison report\n",
        "    report = create_comparison_report(results)\n",
        "\n",
        "    # Save results\n",
        "    output_file = Config.OUTPUT_DIR / \"medgemma_evaluation_report.json\"\n",
        "    with open(output_file, \"w\") as f:\n",
        "        json.dump(report, f, indent=2)\n",
        "\n",
        "    print(f\"\\n✓ Report saved to: {output_file}\")\n",
        "\n",
        "    # GPU stats\n",
        "    print(f\"\\n[GPU Stats]\")\n",
        "    print(f\"  Total allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "    print(f\"  Max allocated: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}